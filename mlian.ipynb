{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't forget install this libraries and models!\n",
    "\n",
    "!pip3 install laserembeddings\n",
    "!python -m laserembeddings download-models\n",
    "\n",
    "!pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import math\n",
    "import time\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from tensorflow.keras.layers import Concatenate, Dense, Input, LSTM, Dropout\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from laserembeddings import Laser\n",
    "laser = Laser()\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "bert = SentenceTransformer('distiluse-base-multilingual-cased-v2')\n",
    "\n",
    "warnings.simplefilter(\"ignore\", UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train data\n",
    "print('Read train data...')\n",
    "en_train = pd.read_csv('hateval2019_en_train.csv')\n",
    "es_train = pd.read_csv('hateval2019_es_train.csv')\n",
    "train = pd.concat([en_train, es_train], axis = 0)\n",
    "\n",
    "print('Load train emdeddings...')\n",
    "X1 = laser.embed_sentences(train['text'], lang = 'ml') # Uncomment if you want to run model with LASER embeddings\n",
    "#X1 = bert.encode([text for text in train['text']]) # Uncomment if you want to run model with DistilBERT embeddings\n",
    "Y1 = to_categorical(train['HS'], 2)\n",
    "Z1 = to_categorical(train['TR'], 2)\n",
    "\n",
    "\n",
    "print('Read test data...')\n",
    "en_test = pd.read_csv('hateval2019_en_dev.csv')\n",
    "es_test = pd.read_csv('hateval2019_es_dev.csv')\n",
    "test = pd.concat([en_test, es_test], axis = 0)\n",
    "\n",
    "print('Load test emdeddings...')\n",
    "X2 = laser.embed_sentences(test['text'], lang = 'ml') # Uncomment if you want to run model with LASER embeddings\n",
    "#X2 = bert.encode([text for text in test['text']]) # Uncomment if you want to run model with DistilBERT embeddings\n",
    "Y2 = to_categorical(test['HS'], 2)\n",
    "Z2 = to_categorical(test['TR'], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSFORMER_SHAPE = 1024 # LASER - 1024; DistilBERT â€” 512\n",
    "\n",
    "# reshape train data\n",
    "X1_reshaped = tf.reshape(X1, [-1, 1, TRANSFORMER_SHAPE])\n",
    "X2_reshaped = tf.reshape(X2, [-1, 1, TRANSFORMER_SHAPE])\n",
    "Y1_reshaped = tf.reshape(Y1, [-1, 1, 2])\n",
    "Y2_reshaped = tf.reshape(Y2, [-1, 1, 2])\n",
    "Z1_reshaped = tf.reshape(Z1, [-1, 1, 2])\n",
    "Z2_reshaped = tf.reshape(Z2, [-1, 1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation function\n",
    "\n",
    "def evaluate(pred, gold):\n",
    "    \"\"\"\n",
    "    evaluate accuracy and macro-F1 of ABSA task\n",
    "    \"\"\"\n",
    "    pred_count = np.zeros(3, dtype='int32')\n",
    "    gold_count = np.zeros(3, dtype='int32')\n",
    "    hit_count = np.zeros(3, dtype='int32')\n",
    "\n",
    "    # number of testing documents\n",
    "    n_test = len(gold)\n",
    "    error_cases = {}\n",
    "    for i in range(n_test):\n",
    "        y_p = int(pred[i])\n",
    "        y_g = gold[i]\n",
    "        pred_count[y_p] += 1\n",
    "        gold_count[y_g] += 1\n",
    "        if y_p == y_g:\n",
    "            hit_count[y_p] += 1\n",
    "        else:\n",
    "            error_cases[i] = [y_p, y_g]\n",
    "    # number of true predictions\n",
    "    total_hit = sum(hit_count)\n",
    "    # accuracy\n",
    "    acc = float(total_hit) / n_test\n",
    "    # macro_f1\n",
    "    macro_f = f1_score(y_true=gold, y_pred=pred, labels=[0, 1], average='weighted')\n",
    "    # auc\n",
    "    auc = roc_auc_score(gold, pred)\n",
    "\n",
    "    result_string = ''\n",
    "    result_string = '%sneg: recall: %s/%s, precision: %s/%s \\n' % (result_string,\n",
    "                                                                   hit_count[0], gold_count[0], hit_count[0],\n",
    "                                                                   pred_count[0])\n",
    "    result_string = '%spos: recall: %s/%s, precision: %s/%s \\n' % (result_string,\n",
    "                                                                   hit_count[1], gold_count[1], hit_count[1],\n",
    "                                                                   pred_count[1])\n",
    "    result_string = '%sneu: recall: %s/%s, precision: %s/%s \\n' % (result_string,\n",
    "                                                                   hit_count[2], gold_count[2], hit_count[2],\n",
    "                                                                   pred_count[2])\n",
    "    return acc, auc, macro_f, result_string, error_cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DROPOUT = 0.1\n",
    "batch_size = 128 \n",
    "n_epoch = 20\n",
    "learning_rate = 0.01\n",
    "\n",
    "class MLIAN(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(MLIAN, self).__init__()\n",
    "\n",
    "        self.n_hidden = 300\n",
    "        self.n_class = 2\n",
    "        self.l2_reg = 0.00001\n",
    "\n",
    "        self.max_aspect_len = TRANSFORMER_SHAPE\n",
    "        self.max_context_len = TRANSFORMER_SHAPE\n",
    "\n",
    "        self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)\n",
    "        self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)\n",
    "\n",
    "        self.aspect_lstm = tf.keras.layers.LSTM(self.n_hidden,\n",
    "                                                return_sequences=True,\n",
    "                                                recurrent_initializer='glorot_uniform', # check this glorot_uniform\n",
    "                                                stateful=True)\n",
    "        self.context_lstm = tf.keras.layers.LSTM(self.n_hidden,\n",
    "                                                return_sequences=True,\n",
    "                                                recurrent_activation='sigmoid', # check this\n",
    "                                                recurrent_initializer='glorot_uniform', # check this glorot_uniform\n",
    "                                                stateful=True)\n",
    "        self.aspect_w = tf.Variable(tf.random.normal([self.n_hidden, self.n_hidden]), name='aspect_w')\n",
    "        self.aspect_b = tf.Variable(tf.zeros([self.n_hidden]), name='aspect_b')\n",
    "\n",
    "        self.context_w = tf.Variable(tf.random.normal([self.n_hidden, self.n_hidden]), name='context_w')\n",
    "        self.context_b = tf.Variable(tf.zeros([self.n_hidden]), name='context_b')\n",
    "\n",
    "        self.output_fc = tf.keras.layers.Dense(self.n_class, kernel_regularizer=tf.keras.regularizers.l2(l=self.l2_reg)) # check l2\n",
    "\n",
    "    def call(self, data, dropout=DROPOUT):\n",
    "        aspects, contexts, labels = data\n",
    "        # aspects = Target parameter\n",
    "        aspect_inputs = tf.cast(aspects, tf.float32)\n",
    "        aspect_inputs = tf.nn.dropout(aspect_inputs, dropout)\n",
    "\n",
    "        # contexts = laser embeddings\n",
    "        context_inputs = tf.cast(contexts, tf.float32)\n",
    "        context_inputs = tf.nn.dropout(context_inputs, dropout)\n",
    "\n",
    "        aspect_outputs = self.aspect_lstm(aspect_inputs)\n",
    "        aspect_avg = tf.reduce_mean(aspect_outputs, 1)\n",
    "\n",
    "        context_outputs = self.context_lstm(context_inputs)\n",
    "        context_avg = tf.reduce_mean(context_outputs, 1)\n",
    "\n",
    "        aspect_att = tf.nn.softmax(tf.math.tanh(tf.einsum('ijk,kl,ilm->ijm', aspect_outputs, self.aspect_w, # check this tf.nn.tanh\n",
    "                                                        tf.expand_dims(context_avg, -1)) + self.aspect_b), axis=1)\n",
    "\n",
    "        aspect_rep = tf.reduce_sum(aspect_att * aspect_outputs, 1)\n",
    "        context_att = tf.nn.softmax(tf.math.tanh(tf.einsum('ijk,kl,ilm->ijm', context_outputs, self.context_w, # check this tf.nn.tanh\n",
    "                                                         tf.expand_dims(aspect_avg, -1)) + self.context_b),\n",
    "                                    axis=1)\n",
    "        context_rep = tf.reduce_sum(context_att * context_outputs, 1)\n",
    "\n",
    "        rep = tf.concat([aspect_rep, context_rep], 1)\n",
    "        predict = self.output_fc(rep)\n",
    "\n",
    "        return predict, labels\n",
    "\n",
    "def run(model, train_data, test_data):\n",
    "    print('Training ...')\n",
    "    max_acc, max_f1, max_auc, step = 0., 0., 0., -1\n",
    "    logdir = '/logs/'\n",
    "\n",
    "    train_data_size = len(train_data[0])\n",
    "    #print(train_data)\n",
    "    train_data = tf.data.Dataset.from_tensor_slices(train_data)\n",
    "    train_data = train_data.shuffle(buffer_size=train_data_size).batch(batch_size, drop_remainder=True)\n",
    "\n",
    "    test_data_size = len(test_data[0])\n",
    "    test_data = tf.data.Dataset.from_tensor_slices(test_data)\n",
    "    test_data = test_data.batch(batch_size, drop_remainder=True)\n",
    "\n",
    "\n",
    "    iterator = tf.compat.v1.data.Iterator.from_structure(tf.compat.v1.data.get_output_types(train_data),\n",
    "                                           tf.compat.v1.data.get_output_shapes(train_data))\n",
    "    optimizer = tf.optimizers.Adamax(learning_rate=learning_rate)\n",
    "    writer = tf.summary.create_file_writer(logdir)\n",
    "    writer.set_as_default()\n",
    "\n",
    "    for i in range(n_epoch):\n",
    "        cost, predict_list, labels_list = 0., [], []\n",
    "        iterator.make_initializer(train_data)\n",
    "        for _ in range(math.floor(train_data_size / batch_size)):\n",
    "            data = iterator.get_next()\n",
    "            with tf.GradientTape() as tape:\n",
    "                predict, labels = model(data, dropout=DROPOUT)\n",
    "                loss_t = tf.nn.softmax_cross_entropy_with_logits(logits=predict, labels=labels)\n",
    "                loss = tf.reduce_mean(loss_t)\n",
    "                cost += tf.reduce_sum(loss_t)\n",
    "            grads = tape.gradient(loss, model.variables)\n",
    "            optimizer.apply_gradients(zip(grads, model.variables))\n",
    "            predict_list.extend(tf.argmax(tf.nn.softmax(predict), 1).numpy())\n",
    "            labels_list.extend(tf.argmax(labels, 1).numpy())\n",
    "        train_acc, train_auc, train_f1, _, _ = evaluate(pred=predict_list, gold=labels_list)\n",
    "        train_loss = cost / train_data_size\n",
    "        tf.summary.scalar('train_loss', train_loss, step=step)\n",
    "        tf.summary.scalar('train_acc', train_acc, step=step)\n",
    "        tf.summary.scalar('train_auc', train_auc, step=step)\n",
    "        tf.summary.scalar('train_f1', train_f1, step=step)\n",
    "\n",
    "        cost, predict_list, labels_list = 0., [], []\n",
    "        iterator.make_initializer(test_data)\n",
    "        for _ in range(math.floor(test_data_size / batch_size)):\n",
    "            data = iterator.get_next()\n",
    "            predict, labels = model(data, dropout=DROPOUT)\n",
    "            loss_t = tf.nn.softmax_cross_entropy_with_logits(logits=predict, labels=labels)\n",
    "            cost += tf.reduce_sum(loss_t)\n",
    "            predict_list.extend(tf.argmax(tf.nn.softmax(predict), 1).numpy())\n",
    "            labels_list.extend(tf.argmax(labels, 1).numpy())\n",
    "        test_acc, test_auc, test_f1, _, _ = evaluate(pred=predict_list, gold=labels_list)\n",
    "        test_loss = cost / test_data_size\n",
    "\n",
    "        tf.summary.scalar('test_loss', test_loss, step=step)\n",
    "        tf.summary.scalar('test_acc', test_acc, step=step)\n",
    "        tf.summary.scalar('test_auc', test_auc, step=step)\n",
    "        tf.summary.scalar('test_f1', test_f1, step=step)\n",
    "\n",
    "        if test_acc + test_auc + test_f1 > max_acc + max_auc + max_f1:\n",
    "            max_acc = test_acc\n",
    "            max_auc = test_auc\n",
    "            max_f1 = test_f1\n",
    "            step = i\n",
    "        print('epoch %s: train-loss=%.6f; train-acc=%.6f; train-auc=%.6f; train-f1=%.6f; test-loss=%.6f; test-acc=%.6f; test-auc=%.6f; test-f1=%.6f.' % (\n",
    "                str(i), train_loss, train_acc, train_auc, train_f1, test_loss, test_acc, test_auc, test_f1))\n",
    "\n",
    "    print('The max accuracy of testing results: acc %.6f, auc %.6f and f1-wighted %.6f of step %s' % (max_acc, max_auc, max_f1, step))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading training data and testing data ...')\n",
    "\n",
    "train_data = Z1_reshaped, X1_reshaped, np.asarray(Y1).astype('float32')\n",
    "test_data = Z2_reshaped, X2_reshaped, np.asarray(Y2).astype('float32')\n",
    "\n",
    "print('Loading model ...')\n",
    "model = MLIAN()\n",
    "start_time = time.time()\n",
    "\n",
    "run(model, train_data, test_data)\n",
    "end_time = time.time()\n",
    "print('Time Costing: %s' % (end_time - start_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
